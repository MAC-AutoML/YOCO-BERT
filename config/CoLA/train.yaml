# dir config
data_dir: '/glue_data/CoLA' # The input data dir. Should contain the .tsv files (or other data files) for the task.

output_dir: "/outputdir/" # The output directory where the model predictions and checkpoints will be written.

cache_dir: "" # Where do you want to store the pre-trained models downloaded from s3

# task config
do_train: True # Whether to run training

do_eval: True # Whether to run eval on the dev set.

evaluate_during_training: True # Rul evaluation during training at each logging step

do_lower_case: True # Set this flag if you are using an uncased model

task_name: "CoLA" # The name of the task

max_seq_length: 128 #The maximum total input sequence length after tokenization. Sequences longerthan this will be truncated, sequences shorter will be padded

# model config
teacher_model: "bert-base-uncased" # Path to pre-trained model or shortcut name

teacher_model_sample_config: "default" # teacher config

super_model: "bert-base-uncased" # Path to super model or shortcut name

model_type: "bert" # model type

config_name: "" # Pretrained config name or path if not the same as model_name

tokenizer_name: "" # Pretrained tokenizer name or path if not the same as model_name

# distill config
alpha: 1.0 # Train loss ratio

beta: 100.0 # Distillation loss ratio

temperature: 5.0 # Distillation temperature for soft target.

# optimizer config


learning_rate: 2e-5 # The initial learning rate for Adam

weight_decay: 0.0 # Weight deay if we apply some

adam_epsilon: 1e-8 # Epsilon for Adam optimizer

max_grad_norm: 1.0 # Max gradient norm

# train/eval config
num_train_epochs: 3.0 # Total number of training epochs to perform

max_steps: -1 # If > 0: set total number of training steps to perform. Override num_train_epochs.

warmup_steps: 0 # Linear warmup over warmup_steps.

per_gpu_train_batch_size: 8 # Batch size per GPU/CPU for training

per_gpu_eval_batch_size: 8 # Batch size per GPU/CPU for evaluation

gradient_accumulation_steps: 1 # Number of updates steps to accumulate before performing a backward/update pass

# other config
logging_steps: 50000 # Log every X updates steps.

save_steps: 5000000 # Save checkpoint every X updates steps

eval_all_checkpoints: False # Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number

no_cuda: False # Avoid using CUDA when available

overwrite_output_dir: True # Overwrite the content of the output directory

overwrite_cache: False # Overwrite the cached training and evaluation sets

seed: 42 # random seed for initialization

fp16: False # Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit

fp16_opt_level: "01" # fp16_opt_level

local_rank: -1 # local_rank

server_ip: "" # For distant debugging

server_port: "" # For distant debugging

# train super config

super_lr_schedule : 'linear'

hidden_dropout_prob : 0.1

attention_probs_dropout_prob : 0.1

train_seq : ['head', 'ffn', 'layer']
train_kind : ['ps', 'ps', 'ps']

# phase config

head0_weight_decay : 0.0
head0_learning_rate : 2e-5
head0_num_train_epochs : 4.0
head0_max_grad_norm : 1.0
head0_super_lr_schedule : 'linear'
head0_alpha : 1.0
head0_beta : 100.0

head1_weight_decay : 0.0
head1_learning_rate : 2e-5
head1_num_train_epochs : 4.0
head1_max_grad_norm : 1.0
head1_super_lr_schedule : 'linear'
head1_alpha : 1.0
head1_beta : 100.0


ffn0_weight_decay : 0.0
ffn0_learning_rate : 2e-5
ffn0_num_train_epochs : 4.0
ffn0_max_grad_norm : 1.0
ffn0_super_lr_schedule : 'linear'
ffn0_alpha : 1.0
ffn0_beta : 100.0

ffn1_weight_decay : 0.0
ffn1_learning_rate : 2e-5
ffn1_num_train_epochs : 4.0
ffn1_max_grad_norm : 1.0
ffn1_super_lr_schedule : 'linear'
ffn1_alpha : 1.0
ffn1_beta : 100.0

ffn2_weight_decay : 0.0
ffn2_learning_rate : 2e-5
ffn2_num_train_epochs : 4.0
ffn2_max_grad_norm : 1.0
ffn2_super_lr_schedule : 'linear'
ffn2_alpha : 1.0
ffn2_beta : 100.0

layer0_weight_decay : 0.0
layer0_learning_rate : 2e-5
layer0_num_train_epochs : 4.0
layer0_max_grad_norm : 1.0
layer0_super_lr_schedule : 'linear'
layer0_alpha : 1.0
layer0_beta : 100.0

layer1_weight_decay : 0.0
layer1_learning_rate : 2e-5
layer1_num_train_epochs : 4.0
layer1_max_grad_norm : 1.0
layer1_super_lr_schedule : 'linear'
layer1_alpha : 1.0
layer1_beta : 100.0

layer2_weight_decay : 0.0
layer2_learning_rate : 2e-5
layer2_num_train_epochs : 4.0
layer2_max_grad_norm : 1.0
layer2_super_lr_schedule : 'linear'
layer2_alpha : 1.0
layer2_beta : 100.0
